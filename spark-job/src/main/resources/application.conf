# Spark configuration
spark {
  app {
    name = "TelecomAnalytics-4.0"
  }
  master = "k8s://https://kubernetes.default.svc"  # For Minikube deployment
  # master = "local[*]"  # For local testing
  executor.memory = "2g"
  driver.memory = "2g"
  
  # Hadoop S3A configuration for MinIO
  hadoop {
    fs.s3a {
      endpoint = "http://minio:9002"
      access.key = "minioadmin"
      secret.key = "minioadmin"
      path.style.access = true
      impl = "org.apache.hadoop.fs.s3a.S3AFileSystem"
      connection.ssl.enabled = false
      aws.credentials.provider = "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    }
  }
  
  # ClickHouse JDBC configuration
  sql {
    sources {
      clickhouse {
        driver = "com.clickhouse.jdbc.ClickHouseDriver"
        url = "jdbc:clickhouse://clickhouse-server:8123/airflow"
        user = "admin"
        password = "clickhouse_admin"
      }
    }
  }
}

# Data source configuration
data {
  source {
    # MinIO (S3 compatible) paths
    minio {
      raw {
        input = "s3a://spark-data/telecom/raw/*.csv"
        input_parquet = "s3a://spark-data/telecom/raw_parquet/"
      }
      processed {
        output = "s3a://spark-data/telecom/processed/"
        output_parquet = "s3a://spark-data/telecom/processed_parquet/"
        output_checkpoint = "s3a://spark-data/telecom/checkpoints/"
      }
      analytics {
        reports = "s3a://spark-data/telecom/reports/"
        metrics = "s3a://spark-data/telecom/metrics/"
      }
    }
    
    # HDFS fallback (if there are files in other system)
    hdfs {
      input = "hdfs://hdfs-namenode:9000/data/telecom/network_equipment/*.csv"
      output = "hdfs://hdfs-namenode:9000/data/telecom/processed/"
    }
  }
}

# ClickHouse configuration
clickhouse {
  url = "jdbc:clickhouse://clickhouse-server:8123/airflow"
  user = "admin"
  password = "clickhouse_admin"
  database = "airflow"
  
  # Connection pool settings
  connection {
    timeout = "30s"
    socket_timeout = "300s"
    data_timeout = "300s"
  }
  
  # Tables
  tables {
    raw {
      network_equipment = "network_equipment_raw"
      customer_data = "customer_data_raw"
    }
    processed {
      aggregated_metrics = "telecom_aggregated_metrics"
      anomaly_results = "anomaly_detection_results"
      kpi_results = "kpi_calculations"
    }
    dimensions {
      time_dimension = "time_dimension"
      customer_dimension = "customer_dimension"
    }
  }
}

# MinIO/S3 configuration
minio {
  endpoint = "http://minio:9002"
  access_key = "minioadmin"
  secret_key = "minioadmin"
  region = "us-east-1"  # MinIO default region
  path_style_access = true
  
  # Buckets
  buckets {
    raw = "spark-data"
    processed = "spark-data"
    analytics = "spark-data"
  }
}

# Application settings
app {
  processing {
    batch {
      interval = "1 hour"
      window {
        hours = 24
      }
    }
    streaming {
      enabled = false
      checkpoint_location = "s3a://spark-data/telecom/checkpoints/"
    }
  }
  
  anomaly {
    detection {
      enabled = true
      threshold = 0.8
      methods = ["zscore", "iqr", "isolation_forest"]
    }
  }
  
  metrics {
    export {
      enabled = true
      formats = ["parquet", "csv", "json"]
      destinations = ["minio", "clickhouse"]
    }
    retention {
      days = 30
    }
  }
  
  # Kubernetes settings (for Spark on Minikube)
  kubernetes {
    namespace = "spark"
    service_account = "spark"
    image = "alexflames77/telecom-analytics:latest"
    image_pull_policy = "IfNotPresent"
  }
}

# Logging configuration
logging {
  level = "INFO"
  spark_log_level = "WARN"
  appender = "FILE"
  
  paths {
    local = "/opt/spark/logs/telecom-analytics.log"
    hdfs = "hdfs://hdfs-namenode:9000/logs/telecom-analytics/"
    s3 = "s3a://spark-data/logs/telecom-analytics/"
  }
}
