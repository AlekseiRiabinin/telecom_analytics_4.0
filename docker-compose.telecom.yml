services:

  # Kafka Brokers
  kafka-1:
    image: bitnami/kafka:3.8.0
    container_name: kafka-1
    ports:
      - "19092:19092"    # External access
      - "19093:19093"    # Controller port
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_KRAFT_CLUSTER_ID=d8ce1515-401e-44d4-a444-1b6dba479047
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_HEAP_OPTS=-Xmx2G -Xms2G
    volumes:
      - ./server-1.properties:/opt/bitnami/kafka/config/server.properties:ro
    networks:
      - kafka-net
    restart: always
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 2GiB
        reservations:
          cpus: '0.25'
          memory: 1GiB

  kafka-2:
    image: bitnami/kafka:3.8.0
    container_name: kafka-2
    ports:
      - "19094:19094"    # External access (changed from 9095)
      - "19095:19095"    # Controller port
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_KRAFT_CLUSTER_ID=d8ce1515-401e-44d4-a444-1b6dba479047
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_HEAP_OPTS=-Xmx2G -Xms2G
    volumes:
      - ./server-2.properties:/opt/bitnami/kafka/config/server.properties:ro
    networks:
      - kafka-net
    restart: always
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 2GiB
        reservations:
          cpus: '0.25'
          memory: 1GiB
  
  # Kafka Producer (smart meter data)
  kafka-producer:
    build:
      context: .
      dockerfile: producer/Dockerfile
    image: alexflames77/kafka_producer:latest
    container_name: kafka-producer
    depends_on:
      - kafka-1
      - kafka-2
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka-1:19092,kafka-2:19094
      - KAFKA_TOPIC=smart_meter_data
    networks:
      - kafka-net
    volumes:
      - ./data/smart_meter_data.json:/app/data/smart_meter_data.json
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512MiB
        reservations:
          cpus: '0.25'
          memory: 256MiB

  clickhouse:
    image: clickhouse/clickhouse-server:25.9
    container_name: clickhouse-server
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
    ports:
      - "8123:8123"  # HTTP API
      - "9000:9000"  # Native interface
    environment:
      - CLICKHOUSE_DB=telecom_analytics
      - CLICKHOUSE_USER=admin
      - CLICKHOUSE_PASSWORD=clickhouse_admin
      - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1
      - CLICKHOUSE_MAX_SERVER_MEMORY_USAGE=0
      - CLICKHOUSE_MAX_MEMORY_USAGE=0
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - clickhouse_log:/var/log/clickhouse-server
    networks:
      - kafka-net
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    restart: unless-stopped
    healthcheck:
      test:
        - CMD
        - clickhouse-client
        - --user=admin
        - --password=clickhouse_admin
        - --query=SELECT 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4GiB
        reservations:
          cpus: '1.0'
          memory: 2GiB

  minio:
    image: minio/minio:RELEASE.2024-04-18T19-09-19Z
    container_name: minio
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - AWS_ENDPOINT_URL=http://minio:9002
    command: server /data --console-address ":9001" --address ":9002"
    volumes:
      - minio-data:/data
    ports:
      - "9002:9002"
      - "9001:9001"
    networks:
      - kafka-net
    restart: always
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1GiB

  minio-setup:
    image: minio/minio:RELEASE.2024-04-18T19-09-19Z
    container_name: minio-setup
    depends_on:
      minio:
        condition: service_started
    entrypoint: |
      /bin/sh -c "
      echo 'Waiting for MinIO to be ready...';
      sleep 25;

      if ! command -v curl >/dev/null 2>&1; then
        echo 'Installing curl...';
        apt-get update && apt-get install -y curl;
      fi
      
      until curl -s http://minio:9002/minio/health/live >/dev/null; do
        echo 'Waiting for MinIO health endpoint...';
        sleep 5;
      done
      
      if command -v mc >/dev/null 2>&1; then
        mc alias set local http://minio:9002 minioadmin minioadmin;
        mc mb local/trino-data-lake || true;
        mc mb local/trino-warehouse || true;
        mc mb local/trino-catalog || true;
        mc mb local/spark-data || true;
        mc mb local/airflow-logs || true;
        mc anonymous set download local/trino-data-lake || true;
        echo 'MinIO buckets initialized successfully';
      else
        echo 'MC client not available, using API calls instead';

        for bucket in trino-data-lake trino-warehouse trino-catalog spark-data airflow-logs; do
          curl -X PUT http://minio:9002/$bucket -H 'Content-Length: 0' || true;
        done
        echo 'Buckets created via API';
      fi
      "
    networks:
      - kafka-net
    restart: on-failure

  postgres:
    image: postgres:15.2
    container_name: postgres
    environment:
      - POSTGRES_DB=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    ports:
      - "5434:5432"
    volumes:
      - postgres-azure-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 5s
      timeout: 5s
      retries: 5    
    networks:
      - kafka-net
    restart: always
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2GiB
        reservations:
          cpus: '0.5'
          memory: 1GiB

  redis:
    image: redis:7.2-alpine
    container_name: redis
    command: redis-server --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - kafka-net
    restart: always
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512MiB

  airflow-webserver:
    build:
      context: ../apps/airflow-app
      dockerfile: Dockerfile
    image: alexflames77/custom-airflow:latest
    container_name: airflow-webserver
    command: webserver
    depends_on:
      - postgres
      - clickhouse
      - minio
      - mssql
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://postgres:postgres@postgres:5432/postgres
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/postgres
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - AIRFLOW__CORE__FERNET_KEY=PakKUDc_578hbrABpAhOs0PMn7RnDBfkgRO03e_tugA=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - AIRFLOW_CONN_HDFS_DEFAULT=hdfs://namenode:8020
      - AIRFLOW_CONN_KAFKA_DEFAULT=kafka://kafka-1:19092,kafka-2:19095
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-master:7077
      - AIRFLOW_CONN_MINIO_DEFAULT=s3://minioadmin:minioadmin@minio:9002
      - AIRFLOW_CONN_CLICKHOUSE_DEFAULT=clickhouse://admin:clickhouse_admin@clickhouse:8123/telecom_analytics
      - AIRFLOW_CONN_MSSQL_DEFAULT=mssql+pyodbc://sa:Admin123!@mssql:1433/telecom_db?driver=ODBC+Driver+18+for+SQL+Server&TrustServerCertificate=yes
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./hadoop-conf:/opt/hadoop/etc/hadoop
      - ./spark/pipelines:/opt/airflow/dags/spark/pipelines
      - ./spark/jars:/opt/airflow/jars
    ports:
      - "8083:8080" 
    networks:
      - kafka-net
    restart: always
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2GiB

  airflow-scheduler:
    image: alexflames77/custom-airflow:latest
    container_name: airflow-scheduler
    command: scheduler
    depends_on:
      - airflow-webserver
      - mssql
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://postgres:postgres@postgres:5432/postgres
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/postgres
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - AIRFLOW__CORE__FERNET_KEY=PakKUDc_578hbrABpAhOs0PMn7RnDBfkgRO03e_tugA=
      - AIRFLOW_CONN_HDFS_DEFAULT=hdfs://namenode:8020
      - AIRFLOW_CONN_KAFKA_DEFAULT=kafka://kafka-1:19092,kafka-2:19095
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-master:7077
      - AIRFLOW_CONN_MINIO_DEFAULT=s3://minioadmin:minioadmin@minio:9002
      - AIRFLOW_CONN_CLICKHOUSE_DEFAULT=clickhouse://admin:clickhouse_admin@clickhouse:8123/telecom_analytics
      - AIRFLOW_CONN_MSSQL_DEFAULT=mssql+pyodbc://sa:Admin123!@mssql:1433/telecom_db?driver=ODBC+Driver+18+for+SQL+Server&TrustServerCertificate=yes
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./hadoop-conf:/opt/hadoop/etc/hadoop
      - ./spark/pipelines:/opt/airflow/dags/spark/pipelines
      - ./spark/jars:/opt/airflow/jars
    networks:
      - kafka-net
    restart: always
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2GiB

  airflow-worker-1:
    image: alexflames77/custom-airflow:latest
    container_name: airflow-worker-1
    command: celery worker
    depends_on:
      - airflow-webserver
      - redis
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://postgres:postgres@postgres:5432/postgres
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/postgres
      - AIRFLOW__CORE__FERNET_KEY=PakKUDc_578hbrABpAhOs0PMn7RnDBfkgRO03e_tugA=
      - AIRFLOW_CONN_HDFS_DEFAULT=hdfs://namenode:8020
      - AIRFLOW_CONN_KAFKA_DEFAULT=kafka://kafka-1:19092,kafka-2:19095
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-master:7077
      - AIRFLOW_CONN_MINIO_DEFAULT=s3://minioadmin:minioadmin@minio:9002
      - AIRFLOW_CONN_CLICKHOUSE_DEFAULT=clickhouse://admin:clickhouse_admin@clickhouse:8123/telecom_analytics
      - AIRFLOW_CONN_MSSQL_DEFAULT=mssql+pyodbc://sa:Admin123!@mssql:1433/telecom_db?driver=ODBC+Driver+18+for+SQL+Server&TrustServerCertificate=yes
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./hadoop-conf:/opt/hadoop/etc/hadoop
      - ./spark/pipelines:/opt/airflow/dags/spark/pipelines
      - ./spark/jars:/opt/airflow/jars
    networks:
      - kafka-net
    restart: always
    deploy:
      resources:
        limits:
          cpus: '0.75'
          memory: 1GiB

  airflow-worker-2:
    image: alexflames77/custom-airflow:latest
    container_name: airflow-worker-2
    command: celery worker
    depends_on:
      - airflow-webserver
      - redis
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://postgres:postgres@postgres:5432/postgres
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/postgres
      - AIRFLOW__CORE__FERNET_KEY=PakKUDc_578hbrABpAhOs0PMn7RnDBfkgRO03e_tugA=
      - AIRFLOW_CONN_HDFS_DEFAULT=hdfs://namenode:8020
      - AIRFLOW_CONN_KAFKA_DEFAULT=kafka://kafka-1:19092,kafka-2:19095
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-master:7077
      - AIRFLOW_CONN_MINIO_DEFAULT=s3://minioadmin:minioadmin@minio:9002
      - AIRFLOW_CONN_CLICKHOUSE_DEFAULT=clickhouse://admin:clickhouse_admin@clickhouse:8123/telecom_analytics
      - AIRFLOW_CONN_MSSQL_DEFAULT=mssql+pyodbc://sa:Admin123!@mssql:1433/telecom_db?driver=ODBC+Driver+18+for+SQL+Server&TrustServerCertificate=yes
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./hadoop-conf:/opt/hadoop/etc/hadoop
      - ./spark/pipelines:/opt/airflow/dags/spark/pipelines
      - ./spark/jars:/opt/airflow/jars
    networks:
      - kafka-net
    restart: always
    deploy:
      resources:
        limits:
          cpus: '0.75'
          memory: 1GiB

  airflow-init:
    image: alexflames77/custom-airflow:latest
    container_name: airflow-init
    entrypoint: /bin/bash
    command: -c "airflow db init && airflow users create \
      --username admin \
      --firstname Admin \
      --lastname User \
      --role Admin \
      --email admin@example.com \
      --password admin"
    depends_on:
      - clickhouse
      - minio
      - mssql
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://postgres:postgres@postgres:5432/postgres
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/postgres
      - AIRFLOW_CONN_HDFS_DEFAULT=hdfs://namenode:8020
      - AIRFLOW_CONN_KAFKA_DEFAULT=kafka://kafka-1:19092,kafka-2:19095
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-master:7077
      - AIRFLOW_CONN_MINIO_DEFAULT=s3://minioadmin:minioadmin@minio:9002
      - AIRFLOW_CONN_CLICKHOUSE_DEFAULT=clickhouse://admin:clickhouse_admin@clickhouse:8123/telecom_analytics
      - AIRFLOW_CONN_MSSQL_DEFAULT=mssql+pyodbc://sa:Admin123!@mssql:1433/telecom_db?driver=ODBC+Driver+18+for+SQL+Server&TrustServerCertificate=yes
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    networks:
      - kafka-net

  mssql:
    image: mcr.microsoft.com/mssql/server:2022-CU12-ubuntu-20.04
    container_name: mssql-server
    environment:
      - ACCEPT_EULA=Y
      - SA_PASSWORD=Admin123!
      - MSSQL_PID=Developer
      - MSSQL_AGENT_ENABLED=true
      - MSSQL_MEMORY_LIMIT_MB=1536
      - MSSQL_TELEMETRY_ENABLED=false
      - MSSQL_COLLATION=SQL_Latin1_General_CP1_CI_AS
    ports:
      - "1433:1433"
    volumes:
      - mssql-data:/var/opt/mssql
      - mssql-backups:/var/opt/mssql/backups
    networks:
      - kafka-net
    restart: unless-stopped
    healthcheck:
      test: /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P "Admin123!" -Q "SELECT 1" -C -b || exit 1
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2GiB
        reservations:
          cpus: '0.5'
          memory: 1GiB

  spark-master:
    image: bitnami/spark:3.5.4
    container_name: spark-master
    hostname: spark-master
    ports:
      - "7077:7077"    # Spark communication port
      - "8091:8080"    # Spark Master Web UI
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_EXTRA_CLASSPATH=/opt/bitnami/spark/custom-jars/*
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_LOG_LEVEL=INFO
    volumes:
      - ./spark/jars:/opt/bitnami/spark/custom-jars
    networks:
      - kafka-net
    restart: always
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2GiB
        reservations:
          cpus: '0.5'
          memory: 1GiB

  spark-worker:
    image: bitnami/spark:3.5.4
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_EXTRA_CLASSPATH=/opt/bitnami/spark/custom-jars/*
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_LOG_LEVEL=INFO
    volumes:
      - ./spark/jars:/opt/bitnami/spark/custom-jars
    depends_on:
      - spark-master
    networks:
      - kafka-net
    restart: always
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 3GiB
        reservations:
          cpus: '0.75'
          memory: 1.5GiB

  debezium-connect:
    build: 
      context: ./debezium
      dockerfile: Dockerfile
    image: alexflames77/custom-debezium:latest
    container_name: debezium-connect
    depends_on:
      - kafka-1
      - kafka-2
      - mssql
    environment:
      - BOOTSTRAP_SERVERS=kafka-1:19092,kafka-2:19094
      - GROUP_ID=debezium-connect-cluster
      - CONFIG_STORAGE_TOPIC=debezium_connect_configs
      - OFFSET_STORAGE_TOPIC=debezium_connect_offsets
      - STATUS_STORAGE_TOPIC=debezium_connect_statuses
      - CONFIG_STORAGE_REPLICATION_FACTOR=2
      - OFFSET_STORAGE_REPLICATION_FACTOR=2
      - STATUS_STORAGE_REPLICATION_FACTOR=2
      - LOG_LEVEL=INFO
      - KAFKA_LOG4J_OPTS=-Dlog4j.configuration=file:/kafka/config/connect-log4j.properties
    ports:
      - "8087:8083"  # Debezium REST API
    networks:
      - kafka-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

volumes:
  clickhouse_data:
    name: clickhouse_data
  clickhouse_log:
    name: clickhouse_log
  minio-data:
    name: minio-data
  postgres-azure-data:
    name: postgres-azure-data
  mssql-data:
    name: mssql-data
  mssql-backups:
    name: mssql-backups
  redis-data:
    name: redis-data

networks:
  kafka-net:
    driver: bridge
